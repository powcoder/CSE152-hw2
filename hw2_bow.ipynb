{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Using Bag-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I:  Review of bag-of-words for image classiﬁcation\n",
    "\n",
    "We will solve image classiﬁcation, with the relatively simple but eﬀective bag-of-words approach. We treat an image as a set of regions and ignore the spatial relationships between diﬀerent regions. The image classiﬁcation problem can then be solved by count the frequencies of diﬀerent regions appearing in an image. Image classiﬁcation using bag-of-words includes the following steps: \n",
    "\n",
    "* **Region Selection**: Select some regions in the images so that we can extract features from these regions in the next step. These regions can be selected by feature detection methods like edge or corner detection, or you can just uniformly sample the image. \n",
    "\n",
    "* **Feature Extraction**: We extract features from the selected regions. One commonly used feature is SIFT. We extract features from every image in the training set. These features are collected to compute the visual vocabulary for image representation. \n",
    "\n",
    "* **Building visual vocabulary**: Once we have the features extracted from training images, we build a visual vocabulary by grouping them together to form a few clusters. We will use k-means algorithm to group the features. The reason why we need this step is to reduce the redundancy in feature space and have a more concise feature representation of images. \n",
    "\n",
    "* **Learning and recognition**: Given the visual vocabulary, each training image is represented by a histogram, where the features in the image populate bins that correspond to the visual word closest to them. Thereafter, we will use k-nearest neighbors to perform classiﬁcation for a test image, also represented by a histogram using the same vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Simple face classifier\n",
    "\n",
    "We will make a classiﬁer to tell whether there is a face in a given image. The dataset contains 200 images with faces and 200 images without faces. We will pick 100 images from each group for training and the other 100 images for testing. The skeleton code is given. Please write your code at the ”WRITE YOUR CODE HERE” prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Parameters\n",
    "posData = 'images/face/'\n",
    "negData = 'images/nonface/'\n",
    "posTrainRatio = 0.5\n",
    "negTrainRatio = 0.5\n",
    "imSize = 133, 200 # resize each image to this size\n",
    "nIntPts = 200 # maximum number of interest points to be extracted from an image\n",
    "wGrid = 5 # width of the small grids for uniform sampling\n",
    "patchSize = 11 # an odd number indicate patch size for image patch feature\n",
    "nCluster = 50 # number of cluster in k-means algorithm\n",
    "\n",
    "def listImage(dataRoot):\n",
    "    fileList = glob.glob(str(dataRoot + '*.jpg'))\n",
    "    imNum = len(fileList)\n",
    "    fileList = [fileList[i].replace('\\\\', '/') for i in range(imNum)]\n",
    "    return fileList\n",
    "\n",
    "def loadImage(imgName, imSize):\n",
    "    # load image, resize and make RGB to grayscale\n",
    "    img = cv2.imread(imgName)\n",
    "    img = cv2.resize(img, (imSize[1], imSize[0]))\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    return img\n",
    "\n",
    "def createSplit(imgList, ratio):\n",
    "    random.shuffle(imgList)\n",
    "    trainList = imgList[:round(len(imgList)*ratio)]\n",
    "    testList = imgList[round(len(imgList)*ratio):]\n",
    "    return trainList, testList\n",
    "\n",
    "# Data preprocessing: Creaete Training / Testing Split\n",
    "posList = listImage(posData)\n",
    "negList = listImage(negData)\n",
    "trainPosList, testPosList = createSplit(posList, posTrainRatio)\n",
    "trainNegList, testNegList = createSplit(negList, negTrainRatio)\n",
    "trainList = trainPosList + trainNegList\n",
    "trainLabel = np.concatenate((np.ones(len(trainPosList)), np.zeros(len(trainNegList))))\n",
    "testList = testPosList + testNegList\n",
    "testLabel = np.concatenate((np.ones(len(testPosList)), np.zeros(len(testNegList))))\n",
    "testPosLabel = np.ones(len(testPosList))\n",
    "testNegLabel = np.zeros(len(testNegList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Extract interest points\n",
    "\n",
    "You are going to try two methods to extract interest points from image: \n",
    "\n",
    "* **(a) Uniformly sample the images.** You can divide the image into regular grids and choose a point in each grid and then uniformly choose nPts number of interest points.\n",
    "* **(b) Sample on corners.** First use the Harris Corner detector to detect corners in the image and then uniformly choose nPts number of interest points. (This has already been implemented for you as an example.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniformSampling(imSize, nPts, wGrid):\n",
    "    ''' Uniformly sample the images.\n",
    "    Args: \n",
    "        imSize: size of images (height, width)\n",
    "        nPts: maximum number of interest points to be extracted\n",
    "        wGrid: width of the small grids\n",
    "    Return:\n",
    "        pts: a list of interest points (Yi, Xi)\n",
    "    '''\n",
    "    #-------------------------------------#\n",
    "    #         WRITE YOUR CODE HERE        #\n",
    "    #-------------------------------------#\n",
    "    return pts\n",
    "\n",
    "def cornerSampling(img, nPts):\n",
    "    '''\n",
    "    Args: \n",
    "        img: image which you want to extract interest points\n",
    "        nPts: maximum number of interest points to be extracted\n",
    "    Return:\n",
    "        pts: a list of interest points (Yi, Xi)\n",
    "    '''\n",
    "    dst = cv2.cornerHarris(img,2,3,0.04)\n",
    "    dst = cv2.dilate(dst,None)\n",
    "    cor = np.zeros(img.shape)\n",
    "    cor[dst>0.01*dst.max()]=[1]\n",
    "    ptsY, ptsX = np.where(cor==1)\n",
    "    num = min(nPts, len(ptsY))\n",
    "    choice = np.random.choice(len(ptsY), num, replace=False)\n",
    "    ptsY = ptsY[choice]\n",
    "    ptsX = ptsX[choice]\n",
    "    pts = [(ptsY[i], ptsX[i]) for i in range(num)]\n",
    "    return pts\n",
    "\n",
    "def plotInterestPoints(img, pts, idx=0):\n",
    "    ptsMap = np.zeros(img.shape)\n",
    "    ptsY = [Y for Y, X in pts]\n",
    "    ptsX = [X for Y, X in pts]\n",
    "    ptsMap[ptsY, ptsX] = 1\n",
    "    plt.figure(idx)\n",
    "    plt.subplot(121),plt.imshow(img,cmap = 'gray')\n",
    "    plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(122),plt.imshow(ptsMap,cmap = 'gray')\n",
    "    plt.title('Interest Points Image'), plt.xticks([]), plt.yticks([])\n",
    "\n",
    "# Here is the code for you to test your implementation\n",
    "sampleImg = loadImage(trainList[0], imSize)\n",
    "print('Interest Points extracted by uniform (above) and edge (below) method:')\n",
    "ptsU = uniformSampling(imSize, nIntPts, wGrid)\n",
    "plotInterestPoints(sampleImg, ptsU, idx=0)\n",
    "ptsE = cornerSampling(sampleImg, nIntPts)\n",
    "plotInterestPoints(sampleImg, ptsE, idx=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Features\n",
    "You are required to try two kinds of features. \n",
    "* **(a) SIFT feature.** Here we use the SIFT implementation in *OpenCV* package (this has already been implemented for you as an example). \n",
    "* **(b) Image Patch feature.** Extract a small image patch around each feature point. You can decide the size of each patch and how many pixels it should cover on you own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractSIFTfeature(img, pts):\n",
    "    '''\n",
    "    Args:\n",
    "        img: input image\n",
    "        pts: detected interest points in the previous step\n",
    "    Return:\n",
    "        features: a list of SIFT descriptor features for each interest point\n",
    "    '''\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp = [cv2.KeyPoint(ptsX, ptsY, 1) for ptsY, ptsX in pts]\n",
    "    _, des = sift.compute(img, kp)\n",
    "    features = [des[i] for i in range(des.shape[0])]\n",
    "    return features\n",
    "\n",
    "def extractImagePatchfeature(img, pts, patchSize):\n",
    "    '''\n",
    "    Args:\n",
    "        img: input image\n",
    "        pts: detected interest points in the previous step\n",
    "        patchSize: an odd number indicate patch size\n",
    "    Return:\n",
    "        features: a list of image patch features (1-d) for each interest point\n",
    "    '''\n",
    "    #-------------------------------------#\n",
    "    #         WRITE YOUR CODE HERE        #\n",
    "    #-------------------------------------#\n",
    "    return features\n",
    "    \n",
    "# Here is the code for you to test your implementation\n",
    "featSIFT = extractSIFTfeature(sampleImg, ptsE)\n",
    "print('length of SIFT feature list', len(featSIFT))\n",
    "print('dimension of feature', featSIFT[0].shape)\n",
    "featPatch = extractImagePatchfeature(sampleImg, ptsE, patchSize)\n",
    "print('length of Image Patch feature list', len(featPatch))\n",
    "print('dimension of feature', featPatch[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build visual vocabulary\n",
    "Use K-means clustering to form a visual vocabulary. You can use the python k-means package. You can decide the number of clusters yourself. The default number of cluster centers in k-means is 50. You can try diﬀerent metrics to do k-means clustering and see which gives the best results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def getImgFeat(img, imSize, nIntPts, wGrid, patchSize, ptType, featType):\n",
    "    ''' Output a list of detected features for an image\n",
    "    Args: \n",
    "        img: image which you want to extract interest points\n",
    "        imSize: size of images (height, width)\n",
    "        nIntPts: maximum number of interest points to be extracted\n",
    "        wGrid: width of the small grids\n",
    "        patchSize: an odd number indicate patch size\n",
    "        ptType: 'uniform' or 'corner' indicates the interest pts sampling method\n",
    "        featType: 'sift' or 'patch' indicates the feature extraction method\n",
    "    Return:\n",
    "        extractFeatList: a list of image patch features (1-d) for each interest point\n",
    "    '''\n",
    "    if ptType == 'uniform':\n",
    "        intPts = uniformSampling(imSize, nIntPts, wGrid)\n",
    "    elif ptType == 'corner':\n",
    "        intPts = cornerSampling(img, nIntPts)\n",
    "    else:\n",
    "        assert False, 'ptType must be either uniform or corner'\n",
    "        \n",
    "    if featType == 'sift':\n",
    "        extractFeatList = extractSIFTfeature(img, intPts)\n",
    "    elif featType == 'patch':\n",
    "        extractFeatList = extractImagePatchfeature(img, intPts, patchSize)\n",
    "    else:\n",
    "        assert False, 'featType must be either sift or patch'\n",
    "        \n",
    "    return extractFeatList\n",
    "\n",
    "def collectFeat(trainList, imSize, nIntPts, wGrid, patchSize, ptType, featType):\n",
    "    ''' Collect extracted features for each image among training data\n",
    "    Args:\n",
    "        trainList: list of images filepath\n",
    "        imSize: size of images (height, width)\n",
    "        nIntPts: maximum number of interest points to be extracted\n",
    "        wGrid: width of the small grids\n",
    "        patchSize: an odd number indicate patch size\n",
    "        ptType: 'uniform' or 'corner' indicates the interest pts sampling method\n",
    "        featType: 'sift' or 'patch' indicates the feature extraction method\n",
    "    Return:\n",
    "        feats: (# of features, dim of feature) array\n",
    "    '''\n",
    "    #-------------------------------------#\n",
    "    #         WRITE YOUR CODE HERE        #\n",
    "    #-------------------------------------#\n",
    "    return feats\n",
    "\n",
    "def formVisualVocab(feats, nCluster):\n",
    "    ''' Use k-means algorithm to find k cluster centers, output k-means model\n",
    "    Args: \n",
    "        feats: list of features collected from training data\n",
    "        nCluster: number of cluster in k-means algorithm\n",
    "    Return:\n",
    "        model: k-means model for following steps\n",
    "    '''\n",
    "    #-------------------------------------#\n",
    "    #         WRITE YOUR CODE HERE        #\n",
    "    #-------------------------------------#\n",
    "    return model\n",
    "\n",
    "# Here is the code for you to test your implementation\n",
    "feats = collectFeat(trainList, imSize, nIntPts, wGrid, patchSize, ptType='uniform', featType='sift')\n",
    "print(feats.shape) # should be (total number of extracted features, dim of feature)\n",
    "model = formVisualVocab(feats, nCluster)\n",
    "centers = model.cluster_centers_\n",
    "print(centers.shape) # should be (nCluster, dim of feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute histogram representation\n",
    "Compute the histogram representation of each image, with bins deﬁned over the visual words in the vocabulary. These histograms are the bag-of-words representations of images that will be used for image classiﬁcation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHistogram(img, model, nCluster, imSize, nIntPts, wGrid, patchSize, ptType, featType):\n",
    "    '''\n",
    "    Calculate histogram representation for single image\n",
    "    Args:\n",
    "        img: input image\n",
    "        model: k-means model from previous step\n",
    "        nCluster: number of cluster in k-means algorithm\n",
    "        imSize: size of images (height, width)\n",
    "        nIntPts: maximum number of interest points to be extracted\n",
    "        wGrid: width of the small grids\n",
    "        patchSize: an odd number indicate patch size\n",
    "        ptType: 'uniform' or 'corner' indicates the interest pts sampling method\n",
    "        featType: 'sift' or 'patch' indicates the feature extraction method\n",
    "    Return:\n",
    "        hist: histogram representation (1-d) for input image\n",
    "    '''\n",
    "    #-------------------------------------#\n",
    "    #         WRITE YOUR CODE HERE        #\n",
    "    #-------------------------------------#\n",
    "    return hist\n",
    "    \n",
    "def computeHistograms(trainList, model, nCluster, imSize, nIntPts, wGrid, patchSize, ptType, featType):\n",
    "    ''' Compute histogram representation for whole training dataset\n",
    "    Args: \n",
    "        trainList: list of images filepath\n",
    "        model: k-means model from formVisualVocab\n",
    "        nCluster: number of cluster in k-means algorithm\n",
    "        imSize: size of images (height, width)\n",
    "        nIntPts: maximum number of interest points to be extracted\n",
    "        wGrid: width of the small grids\n",
    "        patchSize: an odd number indicate patch size\n",
    "        ptType: 'uniform' or 'corner' indicates the interest pts sampling method\n",
    "        featType: 'sift' or 'patch' indicates the feature extraction method\n",
    "    Return:\n",
    "        hists: (# of images, nCluster) array - histogram representations among training dataset\n",
    "    '''\n",
    "    #-------------------------------------#\n",
    "    #         WRITE YOUR CODE HERE        #\n",
    "    #-------------------------------------#\n",
    "    return hists\n",
    "    \n",
    "# Here is the code for you to test your implementation\n",
    "hist = getHistogram(sampleImg, model, nCluster, imSize, nIntPts, wGrid, patchSize, ptType='uniform', featType='sift')\n",
    "print(hist.shape) # should be (nCluster)\n",
    "hists = computeHistograms(trainList, model, nCluster, imSize, nIntPts, wGrid, patchSize, ptType='uniform', featType='sift')\n",
    "print(hists.shape) # should be (# of training images, nCluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. K nearest neighbor classifier\n",
    "After building the visual vocabulary, we now do image classiﬁcation using the nearest neighbors method. Given a new image, ﬁrst represent it using the visual vocabulary and then ﬁnd the closest representation in the training set. The test image can be assumed to be in the same category as its nearest neighbors in the training set. To make the algorithm more robust, you can also try to ﬁnd the ﬁrst K-nearest (K =3 or 5) neighbors. You may try diﬀerent distance metrics when doing K-nearest neighbors search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def KNNclassifier(trainX, trainY, n_neighbors):\n",
    "    ''' Return a KNN model by fitting training data (trainX, trainY)\n",
    "    Args:\n",
    "        trainX: a (# of images, nCluster) array of BoW features for training data\n",
    "        trainY: a (# of images) array of class label for training data\n",
    "        n_neighbors: # of neighbors used in KNN classifier\n",
    "    Return:\n",
    "        model: KNN classifier model\n",
    "    '''\n",
    "    #-------------------------------------#\n",
    "    #         WRITE YOUR CODE HERE        #\n",
    "    #-------------------------------------#\n",
    "    return model\n",
    "\n",
    "def getAccuracy(testX, testY, model):\n",
    "    ''' Output the testing accuracy for KNN classifier model \n",
    "    Args:\n",
    "        testX: a (# of images, nCluster) array of BoW features for testing data\n",
    "        testY: a (# of images) array of class label for testing data\n",
    "        model: KNN classifier model\n",
    "    Return:\n",
    "        accu: accuracy of classification prediction on testing data\n",
    "    '''\n",
    "    #-------------------------------------#\n",
    "    #         WRITE YOUR CODE HERE        #\n",
    "    #-------------------------------------#\n",
    "    return accu\n",
    "\n",
    "# Here is the code for you to test your implementation\n",
    "X = [[0], [1], [2], [3]]\n",
    "y = [0, 0, 1, 1]\n",
    "model = KNNclassifier(X, y, n_neighbors=3)\n",
    "print(model.predict([[1.1]])) # Should be [0]\n",
    "Xp = [[0.5], [2.5]]\n",
    "Yp = [0, 0]\n",
    "acc = getAccuracy(Xp, Yp, model)\n",
    "print(acc) # Should be 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate testing accuracy\n",
    "Finally, we can calculate the testing accuracy of our face classifier. The accuracy should be organized into a 2D table as follows. \n",
    "\n",
    "|      .       |  Uniform |  Uniform |  Corner  |  Corner  |\n",
    "|:------------:|:--------:|:--------:|:--------:|:--------:|\n",
    "|      .       | Positive | Negative | Positive | Negative |\n",
    "| SIFT Feature |     .    |     .    |     .    |     .    |\n",
    "|  Image Patch |     .    |     .    |     .    |     .    |\n",
    "\n",
    "Some of the methods may have poor accuracy. But that is ﬁne, don’t worry too much about accuracy. You will get full credit as long as you can implement and reason about the various methods. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------#\n",
    "#         WRITE YOUR CODE HERE        #\n",
    "#-------------------------------------#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Explain your implementation\n",
    " Brieﬂy explain how you implemented the methods and which parameters you ﬁnd to have a signiﬁcant impact on the ﬁnal performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
